{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### jefan \n",
    "#### begun: Dec 29 2016, updated: Jul 15 2018\n",
    "#### analysis pipeline for \"Results: Visual production training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy\n",
    "import matplotlib\n",
    "from matplotlib import pylab, mlab, pyplot\n",
    "np = numpy\n",
    "plt = pyplot\n",
    "import pandas as pd\n",
    "import json\n",
    "import os, sys\n",
    "import cPickle\n",
    "import random\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.core.pylabtools import figsize, getfigs\n",
    "\n",
    "%matplotlib inline\n",
    "from pylab import *\n",
    "from numpy import *\n",
    "\n",
    "## import custom experiment analysis helpers\n",
    "import experiment_analysis_helpers as ah\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### set up paths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## define data directory (where you put downloaded data)\n",
    "data_dir = './data'\n",
    "\n",
    "## modify python path in order to load in useful variables from current directory\n",
    "CURR_DIR = os.getcwd()\n",
    "if os.path.join(CURR_DIR) not in sys.path:\n",
    "    sys.path.append(CURR_DIR) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## read in experiment data CSV\n",
    "## Note:\n",
    "## \"TITC\" = \"Trained Item, Trained Category\" = \"Trained\"\n",
    "## \"UITC\" = \"Untrained Item, Trained Category\" = \"Near\"\n",
    "## \"UIUC\" = \"Untrained Item, Untrained Category\" = \"Far\"\n",
    "X = pd.read_csv(os.path.join(data_dir,'Pooled_Drawing_ExperimentData1.csv'))\n",
    "X.columns = ['sID','trial','phase','cluster','obj','target','rank','numStrokes','trialDuration','TITC','UITC','UIUC','expid','SS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute Rankdiffs (change in target rank as consequence of training)\n",
    "reload(ah)\n",
    "print 'Computing post-pre differences in target rank ... '\n",
    "Rankdifftitc,Rankdiffuitc,Rankdiffuiuc,Preranktitc, \\\n",
    "Prerankuitc,Prerankuiuc,Postranktitc,Postrankuiuc, \\\n",
    "Postrankuiuc,wid,exp = ah.computeRankdiff(X,'rank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute overall recognition accuracy \n",
    "top1 = sum(X['rank']==1)/len(X)\n",
    "top4 = sum(X['rank']<5)/len(X)\n",
    "top8 = sum(X['rank']<9)/len(X)\n",
    "\n",
    "print('Top 1 accuracy: {}'.format(np.round(top1,3)))\n",
    "print('Top 4 accuracy: {}'.format(np.round(top4,3)))\n",
    "print('Top 8 accuracy: {} '.format(np.round(top8,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Descriptive statistics for rank measure\n",
    "\n",
    "## Any difference between conditions in pre-test? \n",
    "Preranktitc_subj = ah.getSubjMean(Preranktitc)\n",
    "Prerankuitc_subj = ah.getSubjMean(Prerankuitc)\n",
    "Prerankuiuc_subj = ah.getSubjMean(Prerankuiuc)\n",
    "\n",
    "print '###### Mean pretest target rank by condition ######'\n",
    "print 'Trained: ' + str(np.mean(Preranktitc_subj)) + '  sem: '  + str(ah.compute_sem(Preranktitc_subj))\n",
    "print 'Near: ' + str(np.mean(Prerankuitc_subj)) + '  sem: '  + str(ah.compute_sem(Prerankuitc_subj)) \n",
    "print 'Far: ' + str(np.mean(Prerankuiuc_subj)) + '  sem: '  + str(ah.compute_sem(Prerankuiuc_subj))\n",
    "\n",
    "print ''\n",
    "# Average rank in pre-test for each condition, broken down by cue type\n",
    "print '###### Average rank in pre-test for each condition, broken down by cue type (image, verbal) ######'\n",
    "print 'Trained: ', np.mean(Preranktitc_subj[exp==0]),np.mean(Preranktitc_subj[exp==1])\n",
    "print 'Near: ', np.mean(Prerankuitc_subj[exp==0]),np.mean(Prerankuitc_subj[exp==1])\n",
    "print 'Far: ', np.mean(Prerankuiuc_subj[exp==0]),np.mean(Prerankuiuc_subj[exp==1])\n",
    "print ''\n",
    "# Mean and SD in pre-test for each cue-type\n",
    "print '###### Mean and SD in pre-test for each cue-type ######'\n",
    "all_image = np.hstack((Preranktitc_subj[exp==0],Prerankuitc_subj[exp==0],Prerankuiuc_subj[exp==0]))\n",
    "all_verbal = np.hstack((Preranktitc_subj[exp==1],Prerankuitc_subj[exp==1],Prerankuiuc_subj[exp==1]))\n",
    "print 'Image: ', np.mean(all_image),np.std(all_image)\n",
    "print 'Verbal: ', np.mean(all_verbal),np.std(all_verbal)\n",
    "\n",
    "## Check similarity of variances across conditions\n",
    "\n",
    "# subsample UIUC (\"Far\") in order to match # observations in TITC (\"Trained\") and UITC (\"Near\") conditions\n",
    "Rankdiffuiuc_sub = []\n",
    "Prerankuiuc_sub = []\n",
    "for i in range(len(Rankdiffuiuc)):\n",
    "    inds = np.random.RandomState(i).choice(range(8),4,replace=False)    \n",
    "    if i == 0:\n",
    "        Rankdiffuiuc_sub = Rankdiffuiuc[i][inds]\n",
    "        Prerankuiuc_sub = Prerankuiuc[i][inds]        \n",
    "    else:\n",
    "        Rankdiffuiuc_sub = np.vstack((Rankdiffuiuc_sub,Rankdiffuiuc[i][inds]))\n",
    "        Prerankuiuc_sub = np.vstack((Prerankuiuc_sub,Prerankuiuc[i][inds]))        \n",
    "        \n",
    "Rankdiffuiuc_subj_sub = Rankdiffuiuc_sub.mean(1)   \n",
    "Prerankuiuc_subj_sub = Prerankuiuc_sub.mean(1)   \n",
    "\n",
    "print ' '\n",
    "print '###### SD of pretest rank by condition ######'\n",
    "print 'Trained: ', np.round(np.std(Preranktitc_subj),2),'  Near: ', np.round(np.std(Prerankuitc_subj),2), '  Far (subsampled): ',np.round(np.std(Prerankuiuc_subj_sub),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## trial duration \n",
    "print 'trial duration: mean = ' + str(np.mean(X['trialDuration'])) + \\\n",
    "'  sem = ' + str(ah.compute_sem(X['trialDuration'])) + \\\n",
    "'  SD =  ' + str(np.std(X['trialDuration']))\n",
    "print '95% CI: ',  np.percentile(X['trialDuration'],2.5), np.percentile(X['trialDuration'],97.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get training timecourse\n",
    "Ranktrainmat, Traintrialmat, Ranktraintimecourse, \\\n",
    "Top32timecourse, Top16timecourse, Top8timecourse, \\\n",
    "Top4timecourse, Top1timecourse = ah.getTrainingTimecourse(X)\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "## plot training timecourse\n",
    "import seaborn as sns\n",
    "sns.set_context('poster')\n",
    "sns.set_style('whitegrid')\n",
    "fig = plt.figure(figsize(4,8))\n",
    "x = np.arange(1,6,1)\n",
    "y_raw = np.mean(np.transpose(Ranktraintimecourse),1)\n",
    "subjmean = np.mean(np.transpose(Ranktraintimecourse),0)\n",
    "grandmean = np.mean(subjmean)\n",
    "y_gmn = Ranktraintimecourse-np.transpose(np.tile(subjmean,(5,1)))+grandmean\n",
    "y = np.mean(y_gmn,0)\n",
    "sem = np.std(y_gmn,0)/sqrt(y_gmn.shape[0]) # between subject error bars\n",
    "plt.ylim([1,12])\n",
    "plt.xlim([0.8,5.2])\n",
    "plt.xticks(np.arange(1,6))\n",
    "plt.xlabel('training repetition',fontsize=24)\n",
    "plt.ylabel('mean target rank',fontsize=24)\n",
    "plt.tick_params(axis='both', which='major', labelsize=26)\n",
    "plt.tick_params(axis='both', which='minor', labelsize=26)\n",
    "plt.tight_layout()\n",
    "if not os.path.exists('./plots'):\n",
    "    os.makedirs('./plots')\n",
    "plt.savefig('./plots/2_training_meanrank_timecourse_blank.pdf')\n",
    "p = plt.errorbar(x,y,yerr=sem,color=[0.2,0.2,0.2])\n",
    "plt.savefig('./plots/2_training_meanrank_timecourse.pdf')\n",
    "plt.close(fig)\n",
    "\n",
    "def gmn_normalize(dv):    \n",
    "    y_raw = np.mean(np.transpose(dv),1)\n",
    "    subjmean = np.mean(np.transpose(dv),0)\n",
    "    grandmean = np.mean(subjmean)\n",
    "    y_gmn = dv-np.transpose(np.tile(subjmean,(5,1)))+grandmean\n",
    "    y = np.mean(y_gmn,0)\n",
    "    sem = np.std(y_gmn,0)/sqrt(y_gmn.shape[0]) # between subject error bars    \n",
    "    return y,sem\n",
    "\n",
    "fig = plt.figure(figsize(4,8))\n",
    "x = np.arange(1,6,1)\n",
    "y32, sem32 = gmn_normalize(Top32timecourse)\n",
    "y16, sem16 = gmn_normalize(Top16timecourse)\n",
    "y8, sem8 = gmn_normalize(Top8timecourse)\n",
    "y4, sem4 = gmn_normalize(Top4timecourse)\n",
    "y1, sem1 = gmn_normalize(Top1timecourse)\n",
    "plt.axhline(y=(1/64),ls='dashed',color=(0.5,0.5,0.5))\n",
    "plt.ylim([0.,1.])\n",
    "plt.xlim([0.8,5.2])\n",
    "plt.xlabel('training repetition',fontsize=24)\n",
    "plt.ylabel('proportion target rank in top k',fontsize=24)\n",
    "plt.xticks(np.arange(1,6))\n",
    "plt.tick_params(axis='both', which='major', labelsize=26)\n",
    "plt.tick_params(axis='both', which='minor', labelsize=26)\n",
    "plt.tight_layout()\n",
    "if not os.path.exists('./plots'):\n",
    "    os.makedirs('./plots')\n",
    "plt.savefig('./plots/2_training_topk_timecourse_blank.pdf')\n",
    "p = plt.errorbar(x,y1,yerr=sem1,color=[0.,0.,0.],label='top1')\n",
    "plt.savefig('./plots/2_training_top1_timecourse.pdf')\n",
    "p = plt.errorbar(x,y32,yerr=sem32,color=[0.8,0.8,0.8],label='top32')\n",
    "p = plt.errorbar(x,y16,yerr=sem16,color=[0.6,0.6,0.6],label='top16')\n",
    "p = plt.errorbar(x,y8,yerr=sem8,color=[0.4,0.4,0.4],label='top8')\n",
    "p = plt.errorbar(x,y4,yerr=sem4,color=[0.2,0.2,0.2],label='top4')\n",
    "plt.savefig('./plots/2_training_topk_timecourse.pdf')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute change in target rank by condition as function of drawing training\n",
    "\n",
    "Rankdifftitc_subj = ah.getSubjMean(Rankdifftitc)\n",
    "Rankdiffuitc_subj = ah.getSubjMean(Rankdiffuitc)\n",
    "Rankdiffuiuc_subj = ah.getSubjMean(Rankdiffuiuc)\n",
    "print '###### Mean change in target rank by condition ######'\n",
    "print 'Trained: ' + str(np.mean(Rankdifftitc_subj)) + '  sem: '  + str(ah.compute_sem(Rankdifftitc_subj))\n",
    "print 'Near: ' + str(np.mean(Rankdiffuitc_subj)) + '  sem: '  + str(ah.compute_sem(Rankdiffuitc_subj)) \n",
    "print 'Far: ' + str(np.mean(Rankdiffuiuc_subj)) + '  sem: '  + str(ah.compute_sem(Rankdiffuiuc_subj))\n",
    "\n",
    "print ' '\n",
    "print '###### SD of change in target rank (post-pre) by condition ######'\n",
    "print 'Trained: ', np.round(np.std(Rankdifftitc_subj),2), '  Near: ', np.round(np.std(Rankdiffuitc_subj),2), '  Far (subsampled): ', np.round(np.std(Rankdiffuiuc_subj_sub),2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairwise comparisons between conditions\n",
    "import scipy.stats as stats\n",
    "def boot_mean(data,nIter):\n",
    "    boot_mean = []\n",
    "    print '# subjects: ' + str(len(data))\n",
    "    for n in range(nIter):\n",
    "        bootgroup = np.random.RandomState(n).choice(data,size=len(data),replace=True) \n",
    "        boot_mean.append(np.nanmean(bootgroup))\n",
    "    boot_mean = map(np.array,[boot_mean]) \n",
    "    return boot_mean\n",
    "\n",
    "def run_ttest(x):\n",
    "    x = remove_nans(x)\n",
    "    t,p = stats.ttest_1samp(x,0)\n",
    "    print 't:', t, '  p:', p\n",
    "    return t,p\n",
    " \n",
    "def remove_nans(x):\n",
    "    return x[~np.isnan(x)]\n",
    "\n",
    "print '---------- Comparisons against zero --------------'\n",
    "print 'Trained'\n",
    "t,p = run_ttest(Rankdifftitc_subj)\n",
    "print 'Near'\n",
    "t,p = run_ttest(Rankdiffuitc_subj)\n",
    "print 'Far'\n",
    "t,p = run_ttest(Rankdiffuiuc_subj)\n",
    "print '---------- Pairwise comparisons --------------'\n",
    "print 'Trained vs. Near'\n",
    "t,p = run_ttest(Rankdifftitc_subj-Rankdiffuitc_subj)\n",
    "print 'Near vs. Far'\n",
    "t,p = run_ttest(Rankdiffuitc_subj-Rankdiffuiuc_subj)\n",
    "print 'Trained vs. Far'\n",
    "t,p = run_ttest(Rankdifftitc_subj-Rankdiffuiuc_subj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check for normality: Plotting quantile-quantile plot against normal distribution \n",
    "def normalize(x):\n",
    "    mu = np.mean(x)\n",
    "    sd = np.std(x)\n",
    "    return (x-mu)/sd\n",
    "\n",
    "sns.set_context('talk')\n",
    "fig = plt.figure(figsize=(15,3))\n",
    "plt.subplot(1,3,1)\n",
    "data = normalize(Rankdifftitc_subj)\n",
    "res = stats.probplot(data,plot=plt)\n",
    "plt.subplot(1,3,2)\n",
    "data = normalize(Rankdiffuitc_subj)\n",
    "res = stats.probplot(data,plot=plt)\n",
    "plt.subplot(1,3,3)\n",
    "data = normalize(Rankdiffuiuc_subj)\n",
    "res = stats.probplot(data,plot=plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot change in rank for each condition vs. 0\n",
    "X = pd.DataFrame(np.vstack((Rankdifftitc.mean(1),Rankdiffuitc.mean(1),Rankdiffuiuc.mean(1))).transpose())\n",
    "X.columns = ['trained','near','far']\n",
    "\n",
    "sns.set_context('poster')\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "sns.barplot(data=X,ci=68,palette='Dark2')\n",
    "plt.ylim([-2.0,1.0])\n",
    "plt.xlabel('Condition',fontsize=18)\n",
    "plt.ylabel('Overall Rank Difference (post-pre)',fontsize=16)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature Analyses\n",
    "\n",
    "## load between-object, within-condition distances (\"feature distance\", \"separation\")\n",
    "between_item_dists_anch = pd.read_csv(os.path.join(data_dir,'between_obj_within_condition_distances.csv'))\n",
    "between_item_dists_anch = np.vstack((between_item_dists_anch['TITC'].values,\\\n",
    "                                     between_item_dists_anch['UITC'].values,\\\n",
    "                                     between_item_dists_anch['UIUC'].values)).T\n",
    "TvsF_anch = between_item_dists_anch[:,0]-between_item_dists_anch[:,2]\n",
    "NvsF_anch = between_item_dists_anch[:,1]-between_item_dists_anch[:,2]\n",
    "\n",
    "## plot 'em\n",
    "textsize=20\n",
    "data = np.vstack((TvsF_anch,NvsF_anch)).T\n",
    "X = pd.DataFrame(data)\n",
    "X.columns = ['trained','near']\n",
    "\n",
    "sns.set_context('poster')\n",
    "fig = plt.figure(figsize=(2,6))\n",
    "sns.barplot(data=X,ci=68,palette='Dark2')\n",
    "plt.ylabel(r'$\\Delta$ feature distance (post-pre)', fontsize=textsize)\n",
    "plt.ylim([-0.02,0.02])\n",
    "plt.xlabel('Condition',fontsize=18)\n",
    "plt.axhline(0, color='black',linewidth=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# within-item distances (\"feature variability\" or \"sharpening\")\n",
    "\n",
    "within_item_dists = np.load(os.path.join(data_dir,'within_item_distances.npy'))\n",
    "sns.set_style('white')\n",
    "sns.set_context('poster')\n",
    "\n",
    "data = within_item_dists.T\n",
    "X = pd.DataFrame(data)\n",
    "X.columns = ['trained']\n",
    "\n",
    "## plot 'em\n",
    "textsize=20\n",
    "sns.set_context('poster')\n",
    "fig = plt.figure(figsize=(1,6))\n",
    "sns.barplot(data=X,ci=68,palette='Dark2')\n",
    "plt.ylabel(r'$\\Delta$ feature variability (late-early)',fontsize=textsize)\n",
    "plt.ylim([-0.08,0.02])\n",
    "plt.axhline(0, color='black',linewidth=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Static (\"Finished drawing control\") and Dynamic (\"Stroke-by-stroke reconstruction\") experiments\n",
    "\n",
    "## Load in data\n",
    "# order of columns: \n",
    "# ['O_TITC','O_UITC','O_UIUC','D_TITC','D_UITC','D_UIUC','S_TITC','S_UITC','S_UIUC']\n",
    "percepcontrol = pd.read_csv(os.path.join(data_dir,'Rankdiff_submean_PooledDrawDynamicStatic.csv'))\n",
    "pc = percepcontrol\n",
    "o_rdtitc = pc['O_TITC'].values\n",
    "o_rduitc = pc['O_UITC'].values\n",
    "o_rduiuc = pc['O_UIUC'].values\n",
    "\n",
    "d_rdtitc = pc['D_TITC'].values\n",
    "d_rduitc = pc['D_UITC'].values\n",
    "d_rduiuc = pc['D_UIUC'].values\n",
    "\n",
    "s_rdtitc = pc['S_TITC'].values\n",
    "s_rduitc = pc['S_UITC'].values\n",
    "s_rduiuc = pc['S_UIUC'].values\n",
    "\n",
    "rankdiffmat = np.vstack([o_rdtitc,o_rduitc,o_rduiuc,d_rdtitc,d_rduitc,d_rduiuc,s_rdtitc,s_rduitc,s_rduiuc]).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## helpers\n",
    "def compute_sem(a):\n",
    "    \"\"\" return the standard error of the mean \"\"\"\n",
    "    return np.std(a,0)/np.sqrt(shape(a)[0])\n",
    "\n",
    "def compute_gmn(a):\n",
    "    \"\"\" grand-mean-normalize a data matrix \"\"\"\n",
    "    y_raw = a\n",
    "    subjmean = np.mean(a,1)\n",
    "    grandmean = np.mean(subjmean)\n",
    "    y_gmn = a-np.transpose(np.tile(subjmean,(shape(a)[1],1)))+grandmean\n",
    "    m = np.mean(y_gmn,0)\n",
    "    sem = np.std(y_gmn,0)/np.sqrt(shape(y_gmn)[0])\n",
    "    return m,sem,y_gmn\n",
    "\n",
    "def get_boot_CI(data,nIter):\n",
    "    boot_mean = []\n",
    "    for n in range(nIter):\n",
    "        bootgroup = np.random.RandomState(n).choice(data,size=len(data),replace=True) \n",
    "        boot_mean.append(np.nanmean(bootgroup))\n",
    "    boot_mean = map(np.array,[boot_mean]) \n",
    "    return np.mean(boot_mean),np.percentile(boot_mean,2.5), np.percentile(boot_mean,97.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Summary of Within-Experiment T-tests'\n",
    "print ' '\n",
    "print 'Original Drawing'\n",
    "print ' '\n",
    "print 'TITC vs. UIUC (enhancement)'\n",
    "dv = o_rdtitc-o_rduiuc\n",
    "t,p = stats.ttest_1samp(dv,0)\n",
    "print 't = ' + str(t) + '  p = ' + str(p) \n",
    "print 'mean = ' + str(np.mean(dv)) + '  std = ' + str(np.std(dv))\n",
    "print 'UITC vs. UIUC (impairment)'\n",
    "dv = o_rduitc-o_rduiuc\n",
    "t,p = stats.ttest_1samp(dv,0)\n",
    "print 't = ' + str(t) + '  p = ' + str(p) \n",
    "print 'mean = ' + str(np.mean(dv)) + '  std = ' + str(np.std(dv))\n",
    "\n",
    "print 'TITC vs. UITC (trained vs. near)'\n",
    "dv = o_rdtitc-o_rduitc\n",
    "t,p = stats.ttest_1samp(dv,0)\n",
    "print 't = ' + str(t) + '  p = ' + str(p) \n",
    "print 'mean = ' + str(np.mean(dv)) + '  std = ' + str(np.std(dv))\n",
    "\n",
    "\n",
    "print 'TITC vs. 0 (trained vs. 0)'\n",
    "dv = o_rdtitc\n",
    "t,p = stats.ttest_1samp(dv,0)\n",
    "print 't = ' + str(t) + '  p = ' + str(p) \n",
    "print 'mean = ' + str(np.mean(dv)) + '  std = ' + str(np.std(dv))\n",
    "\n",
    "print 'UITC vs. 0 (trained vs. 0)'\n",
    "dv = o_rduitc\n",
    "t,p = stats.ttest_1samp(dv,0)\n",
    "print 't = ' + str(t) + '  p = ' + str(p) \n",
    "print 'mean = ' + str(np.mean(dv)) + '  std = ' + str(np.std(dv))\n",
    "\n",
    "print 'UIUC vs. 0 (trained vs. 0)'\n",
    "dv = o_rduiuc\n",
    "t,p = stats.ttest_1samp(dv,0)\n",
    "print 't = ' + str(t) + '  p = ' + str(p) \n",
    "print 'mean = ' + str(np.mean(dv)) + '  std = ' + str(np.std(dv))\n",
    "\n",
    "print ' '\n",
    "print 'Dynamic Viewing'\n",
    "print 'TITC vs. UIUC (enhancement)'\n",
    "dv = d_rdtitc-d_rduiuc\n",
    "t,p = stats.ttest_1samp(dv,0)\n",
    "print 't = ' + str(t) + '  p = ' + str(p) \n",
    "print 'mean = ' + str(np.mean(dv)) + '  std = ' + str(np.std(dv))\n",
    "\n",
    "print 'UITC vs. UIUC (impairment)'\n",
    "dv = d_rduitc-d_rduiuc\n",
    "t,p = stats.ttest_1samp(dv,0)\n",
    "print 't = ' + str(t) + '  p = ' + str(p) \n",
    "print 'mean = ' + str(np.mean(dv)) + '  std = ' + str(np.std(dv))\n",
    "\n",
    "print 'TITC vs. UITC (trained vs. near)'\n",
    "dv = d_rdtitc-d_rduitc\n",
    "t,p = stats.ttest_1samp(dv,0)\n",
    "print 't = ' + str(t) + '  p = ' + str(p) \n",
    "print 'mean = ' + str(np.mean(dv)) + '  std = ' + str(np.std(dv))\n",
    "\n",
    "print 'TITC vs. 0 (trained vs. 0)'\n",
    "dv = d_rdtitc\n",
    "t,p = stats.ttest_1samp(dv,0)\n",
    "print 't = ' + str(t) + '  p = ' + str(p) \n",
    "print 'mean = ' + str(np.mean(dv)) + '  std = ' + str(np.std(dv))\n",
    "\n",
    "print 'UITC vs. 0 (near vs. 0)'\n",
    "dv = d_rduitc\n",
    "t,p = stats.ttest_1samp(dv,0)\n",
    "print 't = ' + str(t) + '  p = ' + str(p) \n",
    "print 'mean = ' + str(np.mean(dv)) + '  std = ' + str(np.std(dv))\n",
    "\n",
    "print 'UIUC vs. 0 (far vs. 0)'\n",
    "dv = d_rduiuc\n",
    "t,p = stats.ttest_1samp(dv,0)\n",
    "print 't = ' + str(t) + '  p = ' + str(p) \n",
    "print 'mean = ' + str(np.mean(dv)) + '  std = ' + str(np.std(dv))\n",
    "\n",
    "\n",
    "print ' '\n",
    "print 'Static Viewing'\n",
    "print 'TITC vs. UIUC (enhancement)'\n",
    "dv = s_rdtitc-s_rduiuc\n",
    "t,p = stats.ttest_1samp(dv,0)\n",
    "print 't = ' + str(t) + '  p = ' + str(p) \n",
    "print 'mean = ' + str(np.mean(dv)) + '  std = ' + str(np.std(dv))\n",
    "print 'UITC vs. UIUC (impairment)'\n",
    "dv = s_rduitc-s_rduiuc\n",
    "t,p = stats.ttest_1samp(dv,0)\n",
    "print 't = ' + str(t) + '  p = ' + str(p) \n",
    "print 'mean = ' + str(np.mean(dv)) + '  std = ' + str(np.std(dv))\n",
    "\n",
    "print 'TITC vs. UITC (trained vs. near)'\n",
    "dv = s_rdtitc-s_rduitc\n",
    "t,p = stats.ttest_1samp(dv,0)\n",
    "print 't = ' + str(t) + '  p = ' + str(p) \n",
    "print 'mean = ' + str(np.mean(dv)) + '  std = ' + str(np.std(dv))\n",
    "\n",
    "print 'TITC vs. 0 (trained vs. 0)'\n",
    "dv = s_rdtitc\n",
    "t,p = stats.ttest_1samp(dv,0)\n",
    "print 't = ' + str(t) + '  p = ' + str(p) \n",
    "print 'mean = ' + str(np.mean(dv)) + '  std = ' + str(np.std(dv))\n",
    "\n",
    "print 'UITC vs. 0 (near vs. 0)'\n",
    "dv = s_rduitc\n",
    "t,p = stats.ttest_1samp(dv,0)\n",
    "print 't = ' + str(t) + '  p = ' + str(p) \n",
    "print 'mean = ' + str(np.mean(dv)) + '  std = ' + str(np.std(dv))\n",
    "\n",
    "print 'UIUC vs. 0 (far vs. 0)'\n",
    "dv = s_rduiuc\n",
    "t,p = stats.ttest_1samp(dv,0)\n",
    "print 't = ' + str(t) + '  p = ' + str(p) \n",
    "print 'mean = ' + str(np.mean(dv)) + '  std = ' + str(np.std(dv))\n",
    "\n",
    "print ' '\n",
    "\n",
    "o_vs_d = o_rdtitc - d_rdtitc\n",
    "o_vs_s = o_rdtitc - s_rdtitc\n",
    "d_vs_s = d_rdtitc - s_rdtitc\n",
    "\n",
    "print 'Drawing Training (o_titc) vs. Dynamic Training (d_titc)'\n",
    "dv = o_vs_d\n",
    "t,p = stats.ttest_1samp(dv,0)\n",
    "print 't = ' + str(t) + '  p = ' + str(p) \n",
    "print 'mean = ' + str(np.mean(dv)) + '  std = ' + str(np.std(dv))\n",
    "\n",
    "print 'Drawing Training (o_titc) vs. Static Training (s_titc)'\n",
    "dv = o_vs_s\n",
    "t,p = stats.ttest_1samp(dv,0)\n",
    "print 't = ' + str(t) + '  p = ' + str(p) \n",
    "print 'mean = ' + str(np.mean(dv)) + '  std = ' + str(np.std(dv))\n",
    "\n",
    "print 'Dynamic Training (d_titc) vs. Static Training (s_titc)'\n",
    "dv = d_vs_s\n",
    "t,p = stats.ttest_1samp(dv,0)\n",
    "print 't = ' + str(t) + '  p = ' + str(p) \n",
    "print 'mean = ' + str(np.mean(dv)) + '  std = ' + str(np.std(dv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# isolate enhancement vs. impairment\n",
    "# trained category (TITC, UITC) vs. baseline (UIUC)\n",
    "\n",
    "o_enhance = o_rdtitc-o_rduiuc\n",
    "o_impair = o_rduitc-o_rduiuc\n",
    "o_contrast = o_rdtitc-o_rduitc\n",
    "\n",
    "d_enhance = d_rdtitc-d_rduiuc\n",
    "d_impair = d_rduitc-d_rduiuc\n",
    "d_contrast = d_rdtitc-d_rduitc\n",
    "\n",
    "s_enhance = s_rdtitc-s_rduiuc\n",
    "s_impair = s_rduitc-s_rduiuc\n",
    "s_contrast = s_rdtitc-s_rduitc\n",
    "\n",
    "normalizedrdmat = np.hstack([o_enhance,o_impair,d_enhance,d_impair,s_enhance,s_impair])\n",
    "\n",
    "onlytrainingmat = np.hstack([o_enhance,d_enhance,s_enhance])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Baseline against Far condition\n",
    "\n",
    "normalizedrdmat = np.vstack([o_enhance,o_impair,d_enhance,d_impair,s_enhance,s_impair]).T\n",
    "onlytrainingmat = np.vstack([o_enhance,d_enhance,s_enhance]).T\n",
    "\n",
    "# Make rank diff summary bar plot from all 3 experiments (N=588 *3)\n",
    "m = np.mean(normalizedrdmat,0)\n",
    "sem = compute_sem(normalizedrdmat)\n",
    "ind = np.arange(len(m[:2]))\n",
    "width = 0.8\n",
    "colors = plt.cm.Set1(np.linspace(0, 1, 2))\n",
    "\n",
    "fig = plt.figure(figsize(10, 6))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "m,sem,y_gmn = compute_gmn(normalizedrdmat[:,:2])\n",
    "X = pd.DataFrame(y_gmn)\n",
    "X.columns = ['trained','near']\n",
    "sns.barplot(data=X,ci=68)\n",
    "plt.ylim([-2.0,2.0])\n",
    "plt.ylabel(r'$\\Delta$ target rank (post-pre)',fontsize=20)\n",
    "plt.title('drawing',fontsize=20)\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "m,sem,y_gmn = compute_gmn(normalizedrdmat[:,2:4])\n",
    "X = pd.DataFrame(y_gmn)\n",
    "X.columns = ['trained','near']\n",
    "sns.barplot(data=X,ci=68)\n",
    "plt.ylim([-2.0,2.0])\n",
    "plt.ylabel(r'$\\Delta$ target rank (post-pre)',fontsize=20)\n",
    "plt.title('dynamic',fontsize=20)\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "m,sem,y_gmn = compute_gmn(normalizedrdmat[:,4:6])\n",
    "X = pd.DataFrame(y_gmn)\n",
    "X.columns = ['trained','near']\n",
    "sns.barplot(data=X,ci=68)\n",
    "plt.ylim([-2.0,2.0])\n",
    "plt.ylabel(r'$\\Delta$ target rank (post-pre)',fontsize=20)\n",
    "plt.title('static',fontsize=20)\n",
    "\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
